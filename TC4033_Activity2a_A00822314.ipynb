{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea86a060365b1ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Maestría en Inteligencia Artificial Aplicada**\n",
    "### **Curso: ADVANCE MACHINE LEARNING METHODS**\n",
    "## Tecnológico de Monterrey\n",
    "### Dr. José Antonio Cantoral Ceballos\n",
    "\n",
    "## Activity Week 4\n",
    "### **Implementing a FC for ASL Dataset Using PyTorch.**\n",
    "\n",
    "*   Roberto Romero Vielma - A00822314\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b2309",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks using PyTorch\n",
    "<br>\n",
    "\n",
    "#### Activity 2a: Implementing a FC for ASL Dataset using PyTorch\n",
    "<br>\n",
    "\n",
    "\n",
    "- Objective\n",
    "\n",
    "    The primary aim of this activity is to transition from using Numpy for network implementation to utilizing PyTorch, a powerful deep learning framework. You will be replicating the work you did for the ASL dataset in Activity 1b, but this time, you'll implement a your multi layer FC model using PyTorch.\n",
    "    \n",
    "- Instructions\n",
    "\n",
    "    Review Previous Work: Begin by reviewing your Numpy-based Fully Connected Network for the ASL dataset from Activity 1b. Note the architecture, hyperparameters, and performance metrics for comparison.\n",
    "\n",
    "    Introduce PyTorch: If you're new to PyTorch, take some time to familiarize yourself with its basic operations and syntax. You can consult the official documentation or follow online tutorials.\n",
    "\n",
    "    Prepare the ASL Dataset: As before, download and preprocess the Kaggle ASL dataset. \n",
    "\n",
    "    Implement the Network: Design your network architecture tailored for the ASL dataset. Pay special attention to PyTorch modules like nn.Linear() and nn.ReLU().\n",
    "\n",
    "    Train the Model: Implement the training loop, making use of PyTorch's autograd to handle backpropagation. Monitor metrics like loss and accuracy as the model trains.\n",
    "\n",
    "    Analyze and Document: In Markdown cells, discuss the architecture choices, any differences in performance between the Numpy and PyTorch implementations, and insights gained from using a deep learning framework like PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183db241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library and give it the alias 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Import the 'string' module for string-related operations\n",
    "import string\n",
    "\n",
    "# Import the 'pandas' library and give it the alias 'pd' for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import the 'matplotlib.pyplot' module and give it the alias 'plt' for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the 'os' module for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# Set the matplotlib backend to 'inline' for Jupyter Notebook or IPython\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the 'torch' library for deep learning\n",
    "import torch\n",
    "\n",
    "# Import modules for defining neural network architectures (nn) and applying activation functions (F)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3896ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable named 'DATA_PATH' and set it to the path of the data directory.\n",
    "DATA_PATH = './asl_data/'\n",
    "\n",
    "# Read the training data from a CSV file into a Pandas DataFrame.\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "\n",
    "# Read the validation data from a CSV file into a Pandas DataFrame.\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa938e",
   "metadata": {},
   "source": [
    "### Always a good idea to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c149b4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the 'train_df' DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2d1df",
   "metadata": {},
   "source": [
    "### Get training label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4348519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'label' column from the 'train_df' DataFrame and convert it to a NumPy array.\n",
    "y_train = np.array(train_df['label'])\n",
    "\n",
    "# Extract the 'label' column from the 'valid_df' DataFrame and convert it to a NumPy array.\n",
    "y_val = np.array(valid_df['label'])\n",
    "\n",
    "# Delete the 'label' column from the 'train_df' DataFrame.\n",
    "del train_df['label']\n",
    "\n",
    "# Delete the 'label' column from the 'valid_df' DataFrame.\n",
    "del valid_df['label']\n",
    "\n",
    "# Extract the remaining data from 'train_df' as a NumPy array and convert it to float32 data type.\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "\n",
    "# Extract the remaining data from 'valid_df' as a NumPy array and convert it to float32 data type.\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9bed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784)\n",
      "(27455,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape (dimensions) of the 'x_train' NumPy array\n",
    "print(x_train.shape)\n",
    "# Print the shape (dimensions) of the 'y_train' NumPy array\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea87a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7172, 784) (7172,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape (dimensions) of the 'x_val' NumPy array and the 'y_val' NumPy array\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853a323",
   "metadata": {},
   "source": [
    "### `split_val_test` Function Explanation\n",
    "\n",
    "The `split_val_test` function is a Python utility designed to split a dataset into two distinct sets: a validation set and a test set. This function is useful for various machine learning and deep learning tasks, where you need to assess the model's performance on data that it hasn't seen during training.\n",
    "\n",
    "### Parameters\n",
    "- `x`: Feature data (NumPy array or tensor).\n",
    "- `y`: Target labels (NumPy array or tensor).\n",
    "- `pct`: The percentage of data to allocate to the validation set (default is 50%).\n",
    "- `shuffle`: A boolean flag indicating whether the data should be shuffled before splitting (default is True).\n",
    "\n",
    "### Functionality\n",
    "1. **Data Consistency Check**: The function begins by checking if the number of samples in the feature data `x` matches the number of samples in the target labels `y`. This ensures that the data is consistent.\n",
    "\n",
    "2. **Shuffling (Optional)**: If the `shuffle` flag is set to `True`, the function shuffles the data. It does this by creating an array of indices corresponding to the samples, shuffling these indices randomly, and then reordering both the feature data and target labels based on the shuffled indices. This step is typically used to randomize the data order for training and evaluation.\n",
    "\n",
    "3. **Data Splitting**: The function then splits the data into two parts based on the specified percentage `pct`. The first part, referred to as the validation set, contains a portion of the data, and the second part, the test set, contains the remaining data. The split is performed along the first dimension (rows).\n",
    "\n",
    "4. **Return Values**: The function returns the following as a tuple:\n",
    "   - `x_val`: The feature data of the validation set.\n",
    "   - `y_val`: The target labels of the validation set.\n",
    "   - `x_test`: The feature data of the test set.\n",
    "   - `y_test`: The target labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b7edd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    # Check if the number of samples in 'x' matches the number of samples in 'y'\n",
    "    assert x.shape[0] == y.shape[0], 'Number of samples x != number samples y'\n",
    "\n",
    "    # Get the total number of samples\n",
    "    total_samples = x.shape[0]\n",
    "\n",
    "    # If 'shuffle' is True, randomly shuffle the data\n",
    "    if shuffle:\n",
    "        # Create an array of indices from 0 to the number of samples\n",
    "        idxs = np.arange(x.shape[0])\n",
    "\n",
    "        # Shuffle the array of indices randomly\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        # Reorder the data based on the shuffled indices\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]\n",
    "\n",
    "    # Split the data into two parts based on 'pct' (percentage)\n",
    "    # The first part is for the validation set, and the second part is for the test set\n",
    "    x_val = x[:int(total_samples * pct), :]\n",
    "    y_val = y[:int(total_samples * pct)]\n",
    "    x_test = x[int(total_samples * pct):, :]\n",
    "    y_test = y[int(total_samples * pct):]\n",
    "\n",
    "    # Return the validation and test sets as tuples\n",
    "    return x_val, y_val, x_test, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fb6fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the split_val_test function to split the validation data into validation and test sets\n",
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7a02137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the data type of the variable 'y_val'\n",
    "type(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "986ec106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3586, 784) (3586,)\n",
      "(3586, 784) (3586,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape (dimensions) of the 'x_val' NumPy array and the 'y_val' NumPy array\n",
    "print(x_val.shape, y_val.shape)\n",
    "# Print the shape (dimensions) of the 'x_test' NumPy array and the 'y_test' NumPy array\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65bdf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# Create a list 'alphabet' containing all lowercase letters from 'a' to 'z'\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "# Remove the letter 'j' from the list\n",
    "alphabet.remove('j')\n",
    "\n",
    "# Remove the letter 'z' from the list\n",
    "alphabet.remove('z')\n",
    "\n",
    "# Print the length (number of elements) of the 'alphabet' list\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17874be",
   "metadata": {},
   "source": [
    "### Normalise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2ac04",
   "metadata": {},
   "source": [
    "### `normalise` Function Explanation\n",
    "\n",
    "The `normalise` function is a Python utility designed to normalize data by subtracting the mean and dividing by the standard deviation. This normalization process is commonly used in data preprocessing, especially in machine learning and deep learning, to transform data into a standard scale. The function takes the mean and standard deviation as inputs and applies normalization to a given dataset.\n",
    "\n",
    "### Parameters\n",
    "- `x_mean`: The mean value used for normalization.\n",
    "- `x_std`: The standard deviation value used for normalization.\n",
    "- `x_data`: The data to be normalized.\n",
    "\n",
    "### Functionality\n",
    "The function performs the following steps:\n",
    "1. **Normalization**: It calculates the normalized data by subtracting the mean (`x_mean`) from each data point and then dividing by the standard deviation (`x_std`). The result is that the data is scaled so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Return Values**: The normalized data is returned as the output of the function. This normalized data is suitable for use in various machine learning algorithms, as it can improve model training and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a5cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x_mean, x_std, x_data):\n",
    "    # Calculate the normalized data by subtracting the mean and dividing by the standard deviation\n",
    "    normalized_data = (x_data - x_mean) / x_std\n",
    "\n",
    "    # Return the normalized data\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8cf6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of 'x_train'\n",
    "x_mean = x_train.mean()\n",
    "\n",
    "# Calculate the standard deviation of 'x_train'\n",
    "x_std = x_train.std()\n",
    "\n",
    "# Normalize 'x_train' using the calculated mean and standard deviation\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "\n",
    "# Normalize 'x_val' using the same mean and standard deviation as 'x_train'\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "\n",
    "# Normalize 'x_test' using the same mean and standard deviation as 'x_train'\n",
    "x_test = normalise(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0eef77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6268384e-06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of the 'x_train' data\n",
    "x_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0625a",
   "metadata": {},
   "source": [
    "### `normalise` Function Explanation\n",
    "\n",
    "The `normalise` function is a Python utility designed to normalize data by subtracting the mean and dividing by the standard deviation. This normalization process is commonly used in data preprocessing, especially in machine learning and deep learning, to transform data into a standard scale. The function takes the mean and standard deviation as inputs and applies normalization to a given dataset.\n",
    "\n",
    "### Parameters\n",
    "- `x_mean`: The mean value used for normalization.\n",
    "- `x_std`: The standard deviation value used for normalization.\n",
    "- `x_data`: The data to be normalized.\n",
    "\n",
    "### Functionality\n",
    "The function performs the following steps:\n",
    "1. **Normalization**: It calculates the normalized data by subtracting the mean (`x_mean`) from each data point and then dividing by the standard deviation (`x_std`). The result is that the data is scaled so that it has a mean of 0 and a standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4761728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(image):\n",
    "    # Create a figure for the plot with a specified size (5x5 inches)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # Display the image using the 'imshow' function with a grayscale colormap\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    # Turn off axis labels and ticks\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5eb103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the data type of the variable 'x_val'\n",
    "type(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9216b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La imagen muestreada representa un: p\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a random index within the range of 'y_val'\n",
    "rnd_idx = np.random.randint(len(y_val))\n",
    "\n",
    "# Uncomment these lines if you want to print the index and the associated label\n",
    "# print(rnd_idx)\n",
    "# print(y_val[rnd_idx])\n",
    "\n",
    "# Access the label at 'rnd_idx' and use it to look up the corresponding letter in the 'alphabet' list\n",
    "print(f'La imagen muestreada representa un: {alphabet[y_val[rnd_idx]]}')\n",
    "\n",
    "# Use the 'plot_number' function to display the image at index 'rnd_idx', reshaping it to 28x28 pixels\n",
    "plot_number(x_val[rnd_idx].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cfc56",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c833b",
   "metadata": {},
   "source": [
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae3ef9",
   "metadata": {},
   "source": [
    "### Create minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3aaf5",
   "metadata": {},
   "source": [
    "### `create_minibatches` Function Explanation\n",
    "\n",
    "The `create_minibatches` function is a Python utility designed for creating minibatches from a dataset. Minibatches are smaller subsets of a larger dataset, often used in machine learning and deep learning to facilitate training and processing of large amounts of data.\n",
    "\n",
    "### Parameters\n",
    "- `mb_size`: The size of each minibatch (number of samples per minibatch).\n",
    "- `x`: The feature data.\n",
    "- `y`: The target labels.\n",
    "- `shuffle`: A boolean flag indicating whether to shuffle the data (default is True).\n",
    "\n",
    "### Functionality\n",
    "The function performs the following steps:\n",
    "\n",
    "1. **Data Consistency Check**: It checks whether the number of samples in the feature data `x` matches the number of samples in the target labels `y`. This is important to ensure that the data is consistent and can be processed correctly.\n",
    "\n",
    "2. **Shuffling (Optional)**: If the `shuffle` flag is set to `True`, the function randomly shuffles the data. It does this by creating an array of indices corresponding to the data samples, shuffling these indices randomly, and then reordering both the feature data and target labels based on the shuffled indices. Shuffling helps randomize the data order, which can be beneficial during training.\n",
    "\n",
    "3. **Minibatch Creation**: The function creates minibatches by iterating through the data. It starts at the beginning of the data and selects `mb_size` samples at a time. These minibatches are formed by extracting subsets of the data, with each minibatch containing a specified number of samples.\n",
    "\n",
    "4. **Return Value**: The function returns an iterator that generates minibatches. Each minibatch is a tuple containing the feature data and corresponding target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "780beecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    '''\n",
    "    # Check if the number of samples in 'x' matches the number of samples in 'y'\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "\n",
    "    # Get the total number of data samples\n",
    "    total_data = x.shape[0]\n",
    "\n",
    "    # If 'shuffle' is True, shuffle the data\n",
    "    if shuffle:\n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]\n",
    "\n",
    "    # Create and return minibatches by iterating through the data\n",
    "    return ((x[i:i + mb_size], y[i:i + mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8f845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "# Iterate through minibatches created by the 'create_minibatches' function\n",
    "for i, (x, y) in enumerate(create_minibatches(128, x_train, y_train)):\n",
    "    # Print the index 'i'\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12273997",
   "metadata": {},
   "source": [
    "### Now the PyTorch part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbd1415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays 'x_train' and 'y_train' into PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train.copy())\n",
    "y_train_tensor = torch.tensor(y_train.copy())\n",
    "\n",
    "# Convert the NumPy arrays 'x_val' and 'y_val' into PyTorch tensors\n",
    "x_val_tensor = torch.tensor(x_val.copy())\n",
    "y_val_tensor = torch.tensor(y_val.copy())\n",
    "\n",
    "# Convert the NumPy arrays 'x_test' and 'y_test' into PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test.copy())\n",
    "y_test_tensor = torch.tensor(y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "087285a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a CUDA-enabled GPU is available; if not, use CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Print the selected device ('cuda' for GPU or 'cpu' for CPU)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c3ba5",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07003972",
   "metadata": {},
   "source": [
    "### `accuracy` Function Explanation\n",
    "\n",
    "The `accuracy` function is a Python utility designed to evaluate the performance of a machine learning or deep learning model on a given dataset. It calculates the model's accuracy and average cost on the dataset, making it a valuable tool for model evaluation.\n",
    "\n",
    "### Function Parameters\n",
    "- `model`: The machine learning or deep learning model to evaluate.\n",
    "- `x`: Feature data for the dataset.\n",
    "- `y`: Target labels for the dataset.\n",
    "- `mb_size`: The size of each minibatch (number of samples per minibatch).\n",
    "\n",
    "### Functionality\n",
    "The `accuracy` function performs the following tasks:\n",
    "\n",
    "1. **Initialization**: It initializes variables to keep track of the number of correct predictions (`num_correct`), the total number of predictions (`num_total`), and a cost accumulator (`cost`).\n",
    "\n",
    "2. **Model Evaluation Mode**: The function sets the model to evaluation mode using `model.eval()`. In this mode, dropout and batch normalization layers are disabled to ensure consistent evaluation.\n",
    "\n",
    "3. **Device Selection**: It moves the model to the specified device (either GPU or CPU) using `model.to(device=device)`.\n",
    "\n",
    "4. **Gradient Computation Disabling**: To improve performance during evaluation, the function uses `with torch.no_grad()` to disable gradient computation. This is especially helpful when gradients are not needed for evaluation.\n",
    "\n",
    "5. **Minibatch Iteration**: The function iterates through minibatches created by a generator function (presumably named `create_minibatches`). For each minibatch, the following steps are performed:\n",
    "   - The feature data and target labels are moved to the specified device with appropriate data types.\n",
    "   - Model scores are computed for the minibatch.\n",
    "   - The cross-entropy loss for the minibatch is computed and accumulated in the `cost` variable.\n",
    "   - The predicted class (with the highest score) is determined for each data point in the minibatch.\n",
    "   - The number of correct predictions in the minibatch is counted.\n",
    "   - The total number of predictions is incremented.\n",
    "\n",
    "6. **Results**: The function returns the average cost per minibatch and the accuracy as a tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e0f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y, mb_size):\n",
    "    # Initialize variables to count the number of correct predictions and the total number of predictions\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Initialize a variable to accumulate the cost\n",
    "    cost = 0.\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    # Disable gradient computation for better performance\n",
    "    with torch.no_grad():\n",
    "        # Iterate through minibatches created by 'create_minibatches'\n",
    "        for mb, (xi, yi) in enumerate(create_minibatches(mb_size, x, y), 1):\n",
    "            # Move the minibatch data to the specified device with the appropriate data types\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "\n",
    "            # Compute model scores for the minibatch\n",
    "            scores = model(xi)\n",
    "\n",
    "            # Compute the cross-entropy loss for the minibatch and accumulate it in the cost\n",
    "            cost += (F.cross_entropy(scores, yi)).item()\n",
    "\n",
    "            # Find the predicted class (with the highest score) for each data point in the minibatch\n",
    "            _, pred = scores.max(dim=1)  # pred shape (mb_size)\n",
    "\n",
    "            # Count the number of correct predictions in the minibatch\n",
    "            num_correct += (pred == yi.squeeze()).sum()  # pred shape (mb_size), yi shape (mb_size, 1)\n",
    "\n",
    "            # Increment the total number of predictions\n",
    "            num_total += pred.size(0)\n",
    "\n",
    "    # Return the average cost per minibatch and the accuracy\n",
    "    return cost / mb, float(num_correct) / num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c2954",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2321f",
   "metadata": {},
   "source": [
    "### `train` Function Explanation\n",
    "\n",
    "The `train` function is a Python utility designed to train a machine learning or deep learning model over a specified number of epochs. It is a fundamental part of the training process and is responsible for updating the model's parameters based on the training data.\n",
    "\n",
    "### Function Parameters\n",
    "- `model`: The machine learning or deep learning model to be trained.\n",
    "- `optimiser`: The optimization algorithm responsible for updating model parameters.\n",
    "- `mb_size`: The size of each minibatch (number of samples per minibatch).\n",
    "- `epochs`: The number of training epochs (default is 100).\n",
    "\n",
    "### Functionality\n",
    "The `train` function carries out the following tasks:\n",
    "\n",
    "1. **Model Device Placement**: It moves the model to the specified device (either GPU or CPU) using `model.to(device=device)`. This ensures that all model operations and computations are performed on the selected hardware.\n",
    "\n",
    "2. **Training Loop**: The function enters a loop that iterates over the specified number of training epochs. For each epoch, it performs the following steps:\n",
    "\n",
    "   - **Initialization**: Variables are initialized to keep track of the number of correct predictions (`train_correct_num`), the total number of predictions (`train_total`), and an accumulator for the training cost (`train_cost_acum`).\n",
    "\n",
    "   - **Minibatch Iteration**: The function iterates through minibatches created by a generator function (presumably named `create_minibatches`). For each minibatch, it follows these steps:\n",
    "   \n",
    "     - **Training Mode**: The model is set to training mode using `model.train()`. This ensures that operations like dropout and batch normalization layers are active for training.\n",
    "\n",
    "     - **Data Preparation**: The feature data and target labels in the minibatch are moved to the specified device with appropriate data types.\n",
    "\n",
    "     - **Model Forward Pass**: The model computes scores (predictions) for the minibatch.\n",
    "\n",
    "     - **Loss Calculation**: The cross-entropy loss is calculated for the minibatch.\n",
    "\n",
    "     - **Gradient Computation and Model Parameter Update**: The function performs the following steps to update the model parameters:\n",
    "       - Zeroes the gradients using `optimiser.zero_grad()`.\n",
    "       - Backpropagates the loss using `cost.backward()`.\n",
    "       - Updates the model parameters using `optimiser.step()`.\n",
    "\n",
    "     - **Counting Correct Predictions**: The number of correct predictions in the minibatch is counted.\n",
    "\n",
    "     - **Tracking Training Data Metrics**: Metrics such as training accuracy, total training samples, and training cost are updated for the epoch.\n",
    "\n",
    "   - **Validation Evaluation**: After each epoch, the function calculates the validation cost and accuracy using the `accuracy` function. This provides insights into the model's performance on a separate validation dataset.\n",
    "\n",
    "   - **Print Training Progress**: The function prints training progress information at specific intervals (e.g., every 20 epochs). This information includes the current epoch, training cost, validation cost, training accuracy, validation accuracy, and the current learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d0e44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, mb_size, epochs=100):\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_correct_num = 0.\n",
    "        train_total = 0.\n",
    "        train_cost_acum = 0\n",
    "\n",
    "        # Iterate through minibatches for training data\n",
    "        for mb, (xi, yi) in enumerate(create_minibatches(mb_size, x_train_tensor, y_train_tensor), 1):\n",
    "            # Set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # Move the minibatch data to the specified device with the appropriate data types\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "\n",
    "            # Compute model scores for the minibatch\n",
    "            scores = model(xi)\n",
    "\n",
    "            # Calculate the cross-entropy loss\n",
    "            cost = F.cross_entropy(input=scores, target=yi.squeeze())\n",
    "\n",
    "            # Zero the gradients, backpropagate the cost, and update model parameters\n",
    "            optimiser.zero_grad()\n",
    "            cost.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Count the number of correct predictions in the minibatch\n",
    "            train_correct_num += (torch.argmax(scores, dim=1) == yi.squeeze()).sum()\n",
    "            train_total += scores.size(0)\n",
    "            train_cost_acum += cost.item()\n",
    "\n",
    "        # Calculate validation cost and accuracy using the 'accuracy' function\n",
    "        val_cost, val_acc = accuracy(model, x_val_tensor, y_val_tensor, mb_size)\n",
    "\n",
    "        # Calculate training accuracy and cost\n",
    "        train_acc = float(train_correct_num) / train_total\n",
    "        train_cost = train_cost_acum / mb\n",
    "\n",
    "        # Print training progress information (e.g., every 20 epochs)\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch: {epoch}, train cost: {train_cost:.6f}, val cost: {val_cost:.6f},'\n",
    "                  f' train acc: {train_acc:.4f}, val acc: {val_acc:.4f},'\n",
    "                  f' lr: {optimiser.param_groups[0][\"lr\"]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b9243",
   "metadata": {},
   "source": [
    "### Model using Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411fd168",
   "metadata": {},
   "source": [
    "### Neural Network Architectural Choices\n",
    "\n",
    "In the following code, we define and train a neural network for a classification problem, incorporating specific hyperparameters and architectural choices.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- `hidden`: We set `hidden` to 200, which determines the number of hidden units in the neural network. This choice affects the model's capacity and its ability to capture complex patterns.\n",
    "\n",
    "- `lr`: The learning rate (`lr`) is set to 1e-3 (0.001). This parameter is crucial as it influences the step size during optimization, affecting the training process's convergence and stability.\n",
    "\n",
    "- `epochs`: We specify `epochs` as 100, determining the number of training epochs. This controls how many times the entire training dataset is passed forward and backward through the network.\n",
    "\n",
    "- `mb_size`: The mini-batch size (`mb_size`) is chosen to be 128, impacting the number of data samples processed in each forward and backward pass during training.\n",
    "\n",
    "### Neural Network Model\n",
    "\n",
    "- `model1`: An instance of the neural network model is created using PyTorch's `nn.Sequential` container. This model comprises the following layers:\n",
    "    - `nn.Linear`: The input layer with 784 features and 'hidden' (200) hidden units. It's a fully connected layer.\n",
    "    - `nn.Dropout`: A dropout layer is added for regularization to prevent overfitting during training.\n",
    "    - `nn.ReLU`: The Rectified Linear Unit (ReLU) activation function is employed to introduce non-linearity.\n",
    "    - `nn.Linear`: The output layer is configured with 24 output classes, suitable for a multi-class classification problem.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "- `optimiser`: We choose the Adam optimizer to optimize the neural network's parameters during training. We set the learning rate (`lr`) to 1e-3 (0.001), and weight decay is specified as 1e-4 (0.0001) to control the optimization process. Weight decay helps prevent overfitting by adding L2 regularization to the optimizer.\n",
    "\n",
    "### Learning Rate Scheduler\n",
    "\n",
    "- `scheduler`: A One Cycle Learning Rate Scheduler is utilized to dynamically adjust the learning rate during training. This scheduler can potentially improve convergence and prevent divergence. We specify the maximum learning rate as 0.1 and set the number of training epochs as 100.\n",
    "\n",
    "### Training\n",
    "\n",
    "- The neural network model is trained using the `train` function, which leverages the defined optimizer and these specific hyperparameters.\n",
    "\n",
    "These architectural choices collectively define the structure and training process of the neural network for the classification task, incorporating the provided hyperparameter values. Customizing these choices can significantly impact the model's performance and training behavior, depending on the specific problem and dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3d678e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train cost: 0.878369, val cost: 0.744949, train acc: 0.7220, val acc: 0.7780, lr: 0.004000\n",
      "Epoch: 20, train cost: 0.189577, val cost: 2.392642, train acc: 0.9588, val acc: 0.7987, lr: 0.004000\n",
      "Epoch: 40, train cost: 0.174906, val cost: 2.895980, train acc: 0.9662, val acc: 0.7708, lr: 0.004000\n",
      "Epoch: 60, train cost: 0.145940, val cost: 3.027805, train acc: 0.9705, val acc: 0.8101, lr: 0.004000\n",
      "Epoch: 80, train cost: 0.157205, val cost: 3.350077, train acc: 0.9702, val acc: 0.7764, lr: 0.004000\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "hidden = 200  # Number of hidden units in the neural network\n",
    "lr = 1e-3  # Learning rate for the optimizer\n",
    "epochs = 100  # Number of training epochs\n",
    "mb_size = 128  # Minibatch size\n",
    "\n",
    "# Create a neural network model\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=hidden),  # Input layer with 784 features and 'hidden' hidden units\n",
    "    nn.Dropout(),  # Dropout layer for regularization\n",
    "    nn.ReLU(),  # ReLU activation function\n",
    "    nn.Linear(in_features=hidden, out_features=24)  # Output layer with 24 classes (classification problem)\n",
    ")\n",
    "\n",
    "# Choose an optimizer (Adam) and set learning rate and weight decay\n",
    "optimiser = torch.optim.Adam(model1.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler (One Cycle Learning Rate Scheduler)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, 0.1, epochs=epochs, steps_per_epoch=215)\n",
    "\n",
    "# Train the neural network model using the 'train' function\n",
    "train(model1, optimiser, mb_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1942c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8078639152258784\n"
     ]
    }
   ],
   "source": [
    "# Calculate and retrieve the accuracy of the trained neural network model 'model1'\n",
    "# on the test dataset ('x_test_tensor' and 'y_test_tensor') using the specified minibatch size ('mb_size').\n",
    "# The 'accuracy' function returns a tuple, and '[1]' is used to access the second element of the tuple, which is the accuracy value.\n",
    "accuracy_value = accuracy(model1, x_test_tensor, y_test_tensor, mb_size)[1]\n",
    "# Print the accuracy value\n",
    "print(accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae7322",
   "metadata": {},
   "source": [
    "### `predict` Function Explanation\n",
    "\n",
    "The `predict` function is a Python utility designed to make predictions using a trained machine learning or deep learning model. Given input data `x`, it computes predictions using the model and returns the predicted class for each data point.\n",
    "\n",
    "### Function Parameters\n",
    "- `x`: The input data for which predictions are to be made.\n",
    "- `model`: The machine learning or deep learning model used for making predictions.\n",
    "\n",
    "### Functionality\n",
    "The `predict` function carries out the following tasks:\n",
    "\n",
    "1. **Data Device Placement**: It moves the input data `x` to the specified device (either GPU or CPU) using `x.to(device=device, dtype=torch.float32)`. This ensures that the input data is in the appropriate format and on the chosen hardware for model computation.\n",
    "\n",
    "2. **Model Prediction**: The function computes model scores for the input data `x`. The assumption is made that the model's output is a set of scores for each of the possible classes, with the shape of `(mb_size, 10)` (indicating 10 classes).\n",
    "\n",
    "3. **Determining Predicted Class**: For each data point, the function identifies the predicted class by selecting the class with the highest score. This is achieved using `scores.max(dim=1)`, and the resulting array `pred` contains the predicted class for each data point. The shape of `pred` is `(mb_size)`.\n",
    "\n",
    "4. **Return Values**: The function returns the predicted class for each data point in the input data `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fa8f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    # Move the input data 'x' to the specified device (GPU or CPU) with the appropriate data type\n",
    "    x = x.to(device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Compute model scores for the input data 'x'\n",
    "    scores = model(x)  # Assumes the model outputs scores for 10 classes, so the shape is (mb_size, 10)\n",
    "\n",
    "    # Find the predicted class (with the highest score) for each data point\n",
    "    _, pred = scores.max(dim=1)  # 'pred' shape: (mb_size)\n",
    "\n",
    "    # Return the predicted class for each data point\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb4edc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La imagen muestreada representa un: b\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor predicho b\n"
     ]
    }
   ],
   "source": [
    "# Generate a random index within the range of the test dataset\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "\n",
    "# Print the ground truth label for the randomly selected image\n",
    "print(f'La imagen muestreada representa un: {alphabet[y_test[rnd_idx]]}')\n",
    "\n",
    "# Display the selected image\n",
    "plot_number(x_test[rnd_idx].reshape(28, 28))\n",
    "\n",
    "# Make a prediction using the 'predict' function for the selected image\n",
    "pred = predict(x_test_tensor[rnd_idx].reshape(1, -1), model1)\n",
    "\n",
    "# Print the predicted label based on the model's prediction\n",
    "print(f'el valor predicho {alphabet[pred]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27072153",
   "metadata": {},
   "source": [
    "### **Performance Differences: NumPy vs. PyTorch**\n",
    "NumPy and PyTorch exhibit notable performance differences:\n",
    "\n",
    "1. **Speed and Efficiency:**\n",
    "PyTorch, optimized for GPU acceleration, surpasses NumPy in matrix operations and overall training speed. This advantage is crucial for deep learning tasks involving large matrices.\n",
    "\n",
    "2. **Automatic Differentiation:**\n",
    "PyTorch's automatic differentiation simplifies gradient computation for backpropagation. NumPy requires manual gradient computation, which can be error-prone and time-consuming.\n",
    "\n",
    "3. **GPU Support:**\n",
    "PyTorch seamlessly integrates with GPUs, enabling efficient parallel computation. This GPU support boosts performance, while NumPy primarily relies on CPU computation.\n",
    "\n",
    "4. **Deep Learning Building Blocks:**\n",
    "PyTorch provides a wide range of pre-defined layers, loss functions, and optimization algorithms tailored for deep learning, making neural network implementation more intuitive. NumPy often requires implementing these components from scratch or relying on third-party libraries.\n",
    "\n",
    "5. **Community and Ecosystem:**\n",
    "PyTorch benefits from a vibrant community and a rich ecosystem, including pre-trained models, libraries, and tutorials. This extensive support expedites development and problem-solving. While NumPy is powerful, it may have fewer specialized deep learning resources.\n",
    "\n",
    "### **Insights Gained from Using PyTorch**\n",
    "Using PyTorch for deep learning tasks has provided valuable insights and advantages:\n",
    "\n",
    "1. **Dynamic Computational Graphs:**\n",
    "PyTorch employs dynamic computational graphs, allowing for model architecture flexibility. This is particularly useful when working with variable-length sequences or dynamic structures, as the graph is built on-the-fly.\n",
    "\n",
    "2. **Debugging and Visualization:**\n",
    "PyTorch offers excellent debugging and visualization tools. Intermediate values, gradients, and tensors can be inspected during training, facilitating issue diagnosis and model behavior understanding.\n",
    "\n",
    "3. **Transfer Learning and Pre-trained Models:**\n",
    "PyTorch provides access to pre-trained deep learning models, which can be fine-tuned for specific tasks. This accelerates development by leveraging models trained on extensive datasets, such as ImageNet, and adapting them to new problems.\n",
    "\n",
    "4. **Community and Research Integration:**\n",
    "PyTorch is widely adopted in both industry and research, keeping users connected with the latest advancements and research findings in deep learning. This integration with the research community provides updates and collaboration opportunities.\n",
    "\n",
    "5. **Scalability and Deployment:**\n",
    "PyTorch offers solutions for scaling deep learning models to distributed systems and deploying them in production environments. These insights are crucial when transitioning from prototyping to real-world applications.\n",
    "\n",
    "In conclusion, PyTorch's performance advantages, automatic differentiation, GPU support, and deep learning-focused ecosystem make it a powerful choice for deep learning tasks. The insights gained extend beyond performance improvements and encompass enhanced development, debugging, and integration with cutting-edge research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
